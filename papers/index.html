<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <link rel="icon" href="../favicon.png"/>
<!--    <meta name="viewport" content="width=device-width"/>-->
    <meta content="width=device-width,initial-scale=1" name="viewport">
    
		<link href="../_app/immutable/assets/0.68cf1b72.css" rel="stylesheet">
		<link href="../_app/immutable/assets/4.3b9871e4.css" rel="stylesheet">
		<link href="../_app/immutable/assets/AMNH_NavBar.45d8b5f3.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.eca8d74a.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/index.642f0572.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/singletons.01fc16a7.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.47f07e73.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.405bcada.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/4.2c08236f.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/AMNH_NavBar.7f71342d.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/_commonjsHelpers.de833af9.js">
</head>
<body data-sveltekit-preload-data="hover">

		<div style="display: contents;">




<div style="position: fixed; left: 0; right: 0; z-index: 10; top: 0px;">


<nav class="svelte-ehsn6h"><div class="inner svelte-ehsn6h"><div class="mobile-icon svelte-ehsn6h"><div class="middle-line svelte-ehsn6h"></div></div>
            <div style="white-space: nowrap;"><a href="/" class="svelte-ehsn6h">AMHN</a></div>


            <ul class="navbar-list svelte-ehsn6h"><li class="svelte-ehsn6h"><a href="/cfp/" class="svelte-ehsn6h">Call</a>
                    </li><li class="svelte-ehsn6h"><a href="/papers/" class="svelte-ehsn6h">Related Papers</a>
                    </li><li class="svelte-ehsn6h"><a href="/#exploring-hopfield" class="svelte-ehsn6h">Demo</a>
                    </li></ul>


</div>
</nav></div>

<div class="contentWrap"><div class="content"><h1 style="margin:1.5em 0;">Related Papers</h1>
    <div class="flex-container svelte-1dhh5t4"></div></div>
</div>


			
			<script>
				{
					__sveltekit_565f08 = {
						base: new URL("..", location).pathname.slice(0, -1),
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [{"type":"data","data":{committee:[{name:"Sugandha Sharma",email:null,blurb:null,affiliations:["MIT"],headshot:null,url:null},{name:"Hamza Tahir Chaudhry",email:null,blurb:null,affiliations:["Harvard"],headshot:null,url:null},{name:"Jacob Zavatone-Veth",email:null,blurb:null,affiliations:["Harvard"],headshot:null,url:null},{name:"Tom Burns",email:null,blurb:null,affiliations:["OIST"],headshot:null,url:null},{name:"Bao Pham",email:null,blurb:null,affiliations:["RPI"],headshot:null,url:null},{name:"Mikail Khona",email:null,blurb:null,affiliations:["MIT"],headshot:null,url:null},{name:"Hidenori Tanaka",email:null,blurb:null,affiliations:["Harvard"],headshot:null,url:null},{name:"Satyananda Kashyap",email:null,blurb:null,affiliations:["IBM Research"],headshot:null,url:null},{name:"Tankut Can",email:null,blurb:null,affiliations:["IAS"],headshot:null,url:null},{name:"Andrey Gromov",email:null,blurb:null,affiliations:["FAIR, Meta AI"],headshot:null,url:null},{name:"Paolo Glorioso",email:null,blurb:null,affiliations:["Stanford"],headshot:null,url:null},{name:"Andy Keller",email:null,blurb:null,affiliations:["University of Amsterdam"],headshot:null,url:null},{name:"David Lipshutz",email:null,blurb:null,affiliations:["Flatiron Institute"],headshot:null,url:null},{name:"Jascha Achterberg",email:null,blurb:null,affiliations:["Cambridge University"],headshot:null,url:null},{name:"Francisco Acosta",email:null,blurb:null,affiliations:["UCSB"],headshot:null,url:null},{name:"Leo Kozachkov",email:null,blurb:null,affiliations:["MIT"],headshot:null,url:null}],organizers:[{name:"Hilde Kuehne",email:"kuehne@uni-frankfurt.de",blurb:"Prof. Dr. Hilde Kuehne is Head of Computer Vision and Machine Learning at the Computational Vision & Artificial Intelligence Group at the Goethe University Frankfurt, and an affiliated professor at the MIT-IBM Watson AI Lab. Her research focuses on learning without labels and multimodal video understanding.  She has organised various workshops in the field, served as area chair for CVPR and ICCV, and as program chair for WACV. Beyond her work, she is committed to bringing more diversity to STEM, and she is a board member of the Women in Computer Vision Initiative.",affiliations:["Goethe University Frankfurt"],headshot:"kuehne.png",url:"https://hildekuehne.github.io/"},{name:"Daniel D. Lee",email:"ddl46@cornell.edu",blurb:"Daniel is the Tisch University Professor in Electrical and Computer Engineering at Cornell Tech and his research group works on latent variable machine learning models, computational neuroscience, and robotic systems. In the past, he has organized a number of NeurIPS workshops, served as NeurIPS Workshops Chair, NeurIPS Program Chair, and on the NeurIPS Executive Board.",affiliations:["Cornell University"],headshot:"lee.png",url:"https://www.ece.cornell.edu/faculty-directory/daniel-dongyuel-lee"},{name:"Cengiz Pehlevan",email:"cpehlevan@seas.harvard.edu",blurb:"Cengiz is an Assistant Professor of Applied Mathematics at Harvard University. His research interests are in theoretical neuroscience and theory of neural computation. Cengiz has been serving as a NeurIPS area chair for the last few years. He has not organized a workshop in NeurIPS before.",affiliations:["Harvard University"],headshot:"pehlevan.png",url:"https://pehlevan.seas.harvard.edu/people/cengiz-pehlevan"},{name:"Parikshit Ram",email:"parikshit.ram@ibm.com",blurb:"Pari has a broad interest in varied fields of machine learning, from efficient similarity search to bilevel optimization and automated machine learning. Relevant to this workshop, he is working on the use of AMs for various forms of end-to-end differentiable unsupervised learning such as clustering.",affiliations:["IBM Research"],headshot:"ram.png",url:"https://research.ibm.com/people/parikshit-ram"},{name:"Mohammed J. Zaki",email:"zaki@cs.rpi.edu",blurb:"Zaki is a Professor and Department Head of Computer Science at RPI. His research interests focus on novel data mining and machine learning techniques, particularly for learning from graph structured and textual data, with applications in bioinformatics, personal health and financial analytics. He founded the BIOKDD workshop (now in its 22nd year; with SIGKDD). He has extensive experience in organizing workshops and conferences, have chaired all major conferences in data mining, and having served as area chair and SPC for all leading AI/ML conferences. He is a Fellow of the ACM, IEEE, and AAAS.",affiliations:["RPI"],headshot:"zaki.png",url:"http://www.cs.rpi.edu/~zaki/"},{name:"Lenka Zdeborová",email:"lenka.zdeborova@epfl.ch",blurb:"Lenka’s research interests are in applications of concepts from statistical physics, such as advanced mean field methods, replica methods and related message-passing algorithms, to problems in machine learning, signal processing, inference and optimization. She is an experienced organizer of dozens of schools, workshops and conferences in physics, machine learning and their interface.",affiliations:["EPFL"],headshot:"zdeborova.png",url:"https://www.epfl.ch/labs/spoc/prof-lenka-zdeborova/"}],panelists:[{name:"Benjamin Hoover",headshot:"hoover.png",url:"https://bhoov.com",blurb:null,affiliations:["GA Tech","IBM Research"]},{name:"Bao Pham",url:"https://lemon-cmd.github.io/",headshot:"pham.png",blurb:null,affiliations:["RPI"]},{name:"Olawale Onabola",url:"https://mila.quebec/en/person/olawaleonabola/",headshot:"onabola.png",blurb:null,affiliations:["MILA"]}],speakers:[{name:"John Hopfield",url:"http://pni.princeton.edu/john-hopfield",headshot:"hopfield.png",blurb:"John Hopfield is a professor emeritus at Princeton University",affiliations:["Princeton University"]},{name:"Dmitry Krotov",url:"https://mitibmwatsonailab.mit.edu/people/dmitry-krotov/",headshot:"krotov.png",blurb:"Dmitry Krotov is a Research Scientist at IBM Research and the MIT-IBM Watson AI Lab.",affiliations:["IBM Research","MIT-IBM"]},{name:"Sepp Hochreiter",url:"https://www.jku.at/en/institute-for-machine-learning/about-us/team/sepp-hochreiter/",headshot:"hochreiter.png",blurb:"Sepp Hochreiter is a leading researcher at ...",affiliations:["University of Lenz"]},{name:"Ila Fiete",url:"https://mcgovern.mit.edu/profile/ila-fiete/",headshot:"fiete.png",blurb:"???",affiliations:["MIT"]},{name:"Carlo Lucibello",url:"https://carlolucibello.github.io/",headshot:"lucibello.png",blurb:"Carlo Lucibello is ...",affiliations:["Bocconi University"]},{name:"Krzysztof Choromanski",url:"https://www.engineering.columbia.edu/krzysztof-choromanski",headshot:"choromanski.png",blurb:"Krzystof Choromanski is ...",affiliations:["Columbia University"]}],call_for_papers:"\u003C!-- Open links in new tabs -->\n\u003Cbase target=\"_blank\">\n\n\nWe invite submissions on novel research results (theoretical and empirical), software frameworks and abstractions, benchmarks, demos, visualizations, and work-in-progress research. The format of submissions is **4-page papers** (excluding references) submitted to [OpenReview](https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/AMHN). The reviews will not be shared publicly.\n\n## Scope\n\nAssociative memory is defined as a network that can link a set of features into high-dimensional vectors, called memories. Prompted by a large enough subset of features taken from one memory, an animal or an AI network with an associative memory can retrieve the rest of the features belonging to that memory. The diverse human cognitive abilities which involve making appropriate responses to stimulus patterns can often be understood as the operation of an associative memory, with the memories often being distillations and consolidations of multiple experiences rather than merely corresponding to a single event.\n\nIn the world of artificial neural networks, a canonical mathematical model of this phenomenon is the Hopfield network ([Hopfield, 1982](https://www.pnas.org/doi/10.1073/pnas.79.8.2554)). Although often narrowly viewed as a model that can store and retrieve predefined verbatim memories of past events, its contemporary variants make it possible to store consolidated memories turning individual experiences into useful representations of the training data. Such modern variants are often trained using the backpropagation algorithm and often benefit from superior memory storage properties ([Krotov & Hopfield, 2016](https://papers.nips.cc/paper_files/paper/2016/hash/eaae339c4d89fc102edd9dbdb6a28915-Abstract.html)). Contemporary Hopfield networks can be used as submodules in larger AI networks solving a diverse set of tasks. The goal of this workshop is to discuss the existing and emerging developments of these ideas. The research topics of interest at this workshop include (but are not limited to):\n\n- Novel architectures for associative memory, Hopfield Networks, Dense Associative Memories, and related models [[Krotov & Hopfield (2016)](https://papers.nips.cc/paper_files/paper/2016/hash/eaae339c4d89fc102edd9dbdb6a28915-Abstract.html), [Demircigil et al. (2017)](https://arxiv.org/abs/1702.01929), [Ramsauer et al. (2020)](https://arxiv.org/abs/2008.02217), [Millidge et al. (2022)](https://arxiv.org/abs/2202.04557), [Krotov (2021)](https://arxiv.org/abs/2107.06446), [Burns & Fukai (2023)](https://arxiv.org/abs/2305.05179), [Bricken & Pehlevan (2021)](https://arxiv.org/abs/2111.05498)].\n- Hybrid memory augmented architectures, e.g., memory augmented Transformers and RNNs, networks with fast weight updates [[Rae et al. (2019)](https://arxiv.org/abs/1911.05507), [Wu et al. (2022)](https://arxiv.org/abs/2203.08913), [Wang et al. (2023)](https://arxiv.org/abs/2306.07174), [Schlag et al. (2021)](http://proceedings.mlr.press/v139/schlag21a.html)].\n- Energy-based models and their applications [[Hoover et al. (2023a)](https://arxiv.org/abs/2302.07253), [Hoover et al. (2023b)](https://bhoov.com/hamux/), [Ota & Taki (2023)](https://arxiv.org/abs/2304.13061)].\n- Training algorithms for energy-based, or memory-based architectures [[Du & Mordach (2019)](https://arxiv.org/abs/1903.08689)].\n- The connection between associative memory and neuroscience, including both insights from neuroscience for better AI, and AI-inspired neurobiological work [[Krotov & Hopfield (2020)](https://arxiv.org/abs/2008.06996), [Whittington et al. (2021)](https://arxiv.org/abs/2112.04035), [Sharma et al. (2022)](https://arxiv.org/abs/2202.00159), [Tyulmankov et al. (2021)](https://arxiv.org/abs/2110.13976)].\n- Kernel methods and associative memories [[Choromanski et al. (2020)](https://arxiv.org/abs/2009.14794)].\n- Theoretical properties of associative memories with insights from statistical physics, contraction analysis, control theory, and related areas [[Lucibello & Mezard (2023)](https://arxiv.org/abs/2304.14964), [Agliari et al. (2022)](https://arxiv.org/abs/2212.00606)].\n- Multimodal architectures with associative memories.\n- Sequential Hopfield networks for temporal sequences [[Karuvally et al. (2022)](https://arxiv.org/abs/2212.05563), [Chaudhry et al. (2023)](https://arxiv.org/abs/2306.04532)].\n- Other machine learning tasks (such as clustering, dimensionality reduction) with associative memories [[Saha et al. (2023)](https://arxiv.org/abs/2306.03209)].\n- Energy-based Transformers [[Hoover et al. (2023a)](https://arxiv.org/abs/2302.07253)].\n- Applications of associative memories and energy-based models to various data domains, such as language, images, sound, graphs, temporal sequences, computational chemistry and biology [[Widrich et al. (2020)](https://arxiv.org/abs/2007.13505), [Liang et al. (2022)](https://www.frontiersin.org/articles/10.3389/fdata.2022.1044709/full), [Furst et al. (2022)](https://proceedings.neurips.cc/paper_files/paper/2022/hash/8078e76f913e31b8467e85b4c0f0d22b-Abstract-Conference.html), [Bricken et al. (2023)](https://arxiv.org/abs/2303.11934), [Tang & Kopp (2021)](https://arxiv.org/abs/2105.15034), [Sandler et al. (2022)](https://openaccess.thecvf.com/content/CVPR2022/html/Sandler_Fine-Tuning_Image_Transformers_Using_Learnable_Memory_CVPR_2022_paper.html), [Xu et al. (2022)](https://arxiv.org/abs/2208.04441)].\n\n## Submission\n\n**The OpenReview submission is now open**: [link](https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/AMHN)\n\nThe submission deadline is: **September 29**, 2023 23:59 AOE\n\nThe format of submissions is 4-page papers (+ references) using the workshop's LaTeX template [LaTeX template](/amhn2023_latex_template.zip \"download\"). Supplementary materials / appendices after the references are allowed and do not count towards the page limit. Submissions should be anonymized and should not include any identifying information about author identities or affiliations. There are no formal proceedings generated from this workshop. Accepted papers will be made public on OpenReview. The reviewing process will be double-blind. In assessing submitted contributions we will use the same rules for conflicts of interest as are used in the main track NeurIPS conference, e.g. reviewers cannot be from the same organization as authors, recent coauthors cannot review each other's submissions. While we welcome short versions of published papers, preference will be given to new and not yet published work.",related_papers:"@misc{hoover2023Energyb,\n  title = {Energy {{Transformer}}},\n  author = {Hoover, Benjamin and Liang, Yuchen and Pham, Bao and Panda, Rameswar and Strobelt, Hendrik and Chau, Duen Horng and Zaki, Mohammed J. and Krotov, Dmitry},\n  year = {2023},\n  month = feb,\n  number = {arXiv:2302.07253},\n  eprint = {2302.07253},\n  primaryclass = {cond-mat, q-bio, stat},\n  publisher = {{arXiv}},\n  doi = {10.48550/arXiv.2302.07253},\n  url = {http://arxiv.org/abs/2302.07253},\n  urldate = {2023-08-07},\n  archiveprefix = {arxiv},\n  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},\n}\n\n@inproceedings{burns2023simplicial,\n  title = {Simplicial Hopfield Networks},\n  booktitle = {The Eleventh International Conference on Learning Representations},\n  author = {Burns, Thomas F and Fukai, Tomoki},\n  year = {2023},\n  url = {https://openreview.net/forum?id=_QLsH8gatwx},\n}\n\n@article{krotov2023new,\n  title = {A New Frontier for {{Hopfield}} Networks},\n  author = {Krotov, Dmitry},\n  year = {2023},\n  month = jul,\n  journal = {Nature Reviews Physics},\n  volume = {5},\n  number = {7},\n  pages = {366--367},\n  publisher = {{Nature Publishing Group}},\n  issn = {2522-5820},\n  doi = {10.1038/s42254-023-00595-y},\n  url = {https://www.nature.com/articles/s42254-023-00595-y},\n  urldate = {2023-08-07},\n  copyright = {2023 Springer Nature Limited},\n  langid = {english},\n  keywords = {Computer science,Statistical physics},\n}\n\n@misc{ota2023iMixera,\n  title = {{{iMixer}}: Hierarchical {{Hopfield}} Network Implies an Invertible, Implicit and Iterative {{MLP-Mixer}}},\n  shorttitle = {{{iMixer}}},\n  author = {Ota, Toshihiro and Taki, Masato},\n  year = {2023},\n  month = apr,\n  number = {arXiv:2304.13061},\n  eprint = {2304.13061},\n  primaryclass = {cond-mat},\n  publisher = {{arXiv}},\n  doi = {10.48550/arXiv.2304.13061},\n  url = {http://arxiv.org/abs/2304.13061},\n  urldate = {2023-08-07},\n  archiveprefix = {arxiv},\n  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks},\n}\n\n@misc{chaudhry2023Long,\n  title = {Long {{Sequence Hopfield Memory}}},\n  author = {Chaudhry, Hamza Tahir and {Zavatone-Veth}, Jacob A. and Krotov, Dmitry and Pehlevan, Cengiz},\n  year = {2023},\n  month = jun,\n  number = {arXiv:2306.04532},\n  eprint = {2306.04532},\n  primaryclass = {cs},\n  publisher = {{arXiv}},\n  doi = {10.48550/arXiv.2306.04532},\n  url = {http://arxiv.org/abs/2306.04532},\n  urldate = {2023-08-07},\n  archiveprefix = {arxiv},\n  keywords = {Computer Science - Neural and Evolutionary Computing},\n}\n\n@misc{lucibello2023Exponential,\n  title = {The {{Exponential Capacity}} of {{Dense Associative Memories}}},\n  author = {Lucibello, Carlo and M{\\'e}zard, Marc},\n  year = {2023},\n  month = jul,\n  number = {arXiv:2304.14964},\n  eprint = {2304.14964},\n  primaryclass = {cond-mat},\n  publisher = {{arXiv}},\n  doi = {10.48550/arXiv.2304.14964},\n  url = {http://arxiv.org/abs/2304.14964},\n  urldate = {2023-08-07},\n  archiveprefix = {arxiv},\n  keywords = {Computer Science - Information Theory,Condensed Matter - Disordered Systems and Neural Networks},\n}\n\n\n@misc{saha2023Endtoend,\n  title = {End-to-End {{Differentiable Clustering}} with {{Associative Memories}}},\n  author = {Saha, Bishwajit and Krotov, Dmitry and Zaki, Mohammed J. and Ram, Parikshit},\n  year = {2023},\n  month = jun,\n  number = {arXiv:2306.03209},\n  eprint = {2306.03209},\n  primaryclass = {cs},\n  publisher = {{arXiv}},\n  doi = {10.48550/arXiv.2306.03209},\n  url = {http://arxiv.org/abs/2306.03209},\n  urldate = {2023-08-07},\n  archiveprefix = {arxiv},\n  keywords = {Computer Science - Machine Learning},\n}\n\n@misc{tang2023Sequential,\n  title = {Sequential {{Memory}} with {{Temporal Predictive Coding}}},\n  author = {Tang, Mufeng and Barron, Helen and Bogacz, Rafal},\n  year = {2023},\n  month = may,\n  number = {arXiv:2305.11982},\n  eprint = {2305.11982},\n  primaryclass = {cs, q-bio, stat},\n  publisher = {{arXiv}},\n  doi = {10.48550/arXiv.2305.11982},\n  url = {http://arxiv.org/abs/2305.11982},\n  urldate = {2023-08-07},\n  archiveprefix = {arxiv},\n  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},\n}\n\n\n@misc{f{\\\"u}rst2022cloob,\n  title = {{{CLOOB}}: {{Modern}} Hopfield Networks with {{InfoLOOB}} Outperform {{CLIP}}},\n  author = {F{\\\"u}rst, Andreas and Rumetshofer, Elisabeth and Tran, Viet Thuong and Ramsauer, Hubert and Tang, Fei and Lehner, Johannes and Kreil, D P and Kopp, Michael K and Klambauer, G{\\\"u}nter and {Bitto-Nemling}, Angela and Hochreiter, Sepp},\n  year = {2022},\n  url = {https://openreview.net/forum?id=qw674L9PfQE},\n}\n\n@misc{karuvally2022Energybaseda,\n  title = {Energy-Based {{General Sequential Episodic Memory Networks}} at the {{Adiabatic Limit}}},\n  author = {Karuvally, Arjun and Sejnowski, Terry J. and Siegelmann, Hava T.},\n  year = {2022},\n  month = dec,\n  number = {arXiv:2212.05563},\n  eprint = {2212.05563},\n  primaryclass = {cs},\n  publisher = {{arXiv}},\n  doi = {10.48550/arXiv.2212.05563},\n  url = {http://arxiv.org/abs/2212.05563},\n  urldate = {2023-08-07},\n  archiveprefix = {arxiv},\n  keywords = {Computer Science - Neural and Evolutionary Computing},\n}\n\n@article{liang2022modern,\n  title = {Modern Hopfield Networks for Graph Embedding},\n  author = {Liang, Yuchen and Krotov, Dmitry and Zaki, Mohammed J},\n  year = {2022},\n  journal = {Frontiers in big Data},\n  volume = {5},\n  pages = {1044709},\n  publisher = {{Frontiers}},\n  url = {https://www.frontiersin.org/articles/10.3389/fdata.2022.1044709/full},\n}\n\n@inproceedings{pmlr-v162-millidge22a,\n  title = {Universal Hopfield Networks: {{A}} General Framework for Single-Shot Associative Memory Models},\n  booktitle = {Proceedings of the 39th International Conference on Machine Learning},\n  author = {Millidge, Beren and Salvatori, Tommaso and Song, Yuhang and Lukasiewicz, Thomas and Bogacz, Rafal},\n  editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},\n  year = {2022},\n  month = jul,\n  series = {Proceedings of Machine Learning Research},\n  volume = {162},\n  pages = {15561--15583},\n  publisher = {{PMLR}},\n  url = {https://proceedings.mlr.press/v162/millidge22a.html},\n}\n@inproceedings{sharma2022content,\n  title = {Content Addressable Memory without Catastrophic Forgetting by Heteroassociation with a Fixed Scaffold},\n  booktitle = {International Conference on Machine Learning},\n  author = {Sharma, Sugandha and Chandra, Sarthak and Fiete, Ila},\n  year = {2022},\n  pages = {19658--19682},\n  publisher = {{PMLR}},\n  url = {https://proceedings.mlr.press/v162/sharma22b/sharma22b.pdf},\n}\n\n@inproceedings{whittington2022relating,\n  title = {Relating Transformers to Models and Neural Representations of the Hippocampal Formation},\n  booktitle = {International Conference on Learning Representations},\n  author = {Whittington, James C. R. and Warren, Joseph and Behrens, Tim E.J.},\n  year = {2022},\n  url = {https://openreview.net/forum?id=B8DVo9B1YE0},\n}\n\n@inproceedings{ramsauer2021hopfield,\n  title = {Hopfield Networks Is All You Need},\n  booktitle = {International Conference on Learning Representations},\n  author = {Ramsauer, Hubert and Sch{\\\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Gruber, Lukas and Holzleitner, Markus and Adler, Thomas and Kreil, David and Kopp, Michael K and Klambauer, G{\\\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},\n  year = {2021},\n  url = {https://openreview.net/forum?id=tL89RnzIiCd},\n}\n\n@inproceedings{krotov2021large,\n  title = {Large Associative Memory Problem in Neurobiology and Machine Learning},\n  booktitle = {International Conference on Learning Representations},\n  author = {Krotov, Dmitry and Hopfield, John J.},\n  year = {2021},\n  url = {https://openreview.net/forum?id=X4y_10OX-hX},\n}\n\n@inproceedings{onabola-etal-2021-hbert,\n  title = {{{hBERT}} + {{BiasCorp}} - Fighting Racism on the Web},\n  booktitle = {Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion},\n  author = {Onabola, Olawale and Ma, Zhuang and Yang, Xie and Akera, Benjamin and Abdulrahman, Ibraheem and Xue, Jia and Liu, Dianbo and Bengio, Yoshua},\n  year = {2021},\n  month = apr,\n  pages = {26--33},\n  publisher = {{Association for Computational Linguistics}},\n  address = {{Kyiv}},\n  url = {https://aclanthology.org/2021.ltedi-1.4},\n}\n\n@article{widrich2020modern,\n  title = {Modern Hopfield Networks and Attention for Immune Repertoire Classification},\n  author = {Widrich, Michael and Sch{\\\"a}fl, Bernhard and Pavlovi{\\'c}, Milena and Ramsauer, Hubert and Gruber, Lukas and Holzleitner, Markus and Brandstetter, Johannes and Sandve, Geir Kjetil and Greiff, Victor and Hochreiter, Sepp and others},\n  year = {2020},\n  journal = {Advances in Neural Information Processing Systems},\n  volume = {33},\n  pages = {18832--18845},\n  url = {https://arxiv.org/abs/2007.13505},\n}\n@inproceedings{NIPS2017_3f5ee243,\n  title = {Attention Is All You Need},\n  booktitle = {Advances in Neural Information Processing Systems},\n  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia},\n  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},\n  year = {2017},\n  volume = {30},\n  publisher = {{Curran Associates, Inc.}},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}\n}\n\n@article{demircigil2017model,\n  title = {On a Model of Associative Memory with Huge Storage Capacity},\n  author = {Demircigil, Mete and Heusel, Judith and L{\\\"o}we, Matthias and Upgang, Sven and Vermet, Franck},\n  year = {2017},\n  journal = {Journal of Statistical Physics},\n  volume = {168},\n  pages = {288--299},\n  publisher = {{Springer}},\n  url = {https://arxiv.org/abs/1702.01929},\n}\n\n@inproceedings{NIPS2016_eaae339c,\n  title = {Dense Associative Memory for Pattern Recognition},\n  booktitle = {Advances in Neural Information Processing Systems},\n  author = {Krotov, Dmitry and Hopfield, John J.},\n  editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},\n  year = {2016},\n  volume = {29},\n  publisher = {{Curran Associates, Inc.}},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/eaae339c4d89fc102edd9dbdb6a28915-Paper.pdf},\n}\n\n@article{hopfield1984Neurons,\n  title = {Neurons {{With Graded Response Have Collective Computational Properties Like Those}} of {{Two-State Neurons}}},\n  author = {Hopfield, John},\n  year = {1984},\n  month = jun,\n  journal = {Proceedings of the National Academy of Sciences of the United States of America},\n  volume = {81},\n  pages = {3088--92},\n  doi = {10.1073/pnas.81.10.3088},\n  url = {https://www.pnas.org/doi/10.1073/pnas.81.10.3088},\n}\n@article{hopfield1982Neurala,\n  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities.},\n  author = {Hopfield, J J},\n  year = {1982},\n  month = apr,\n  journal = {Proceedings of the National Academy of Sciences},\n  volume = {79},\n  number = {8},\n  pages = {2554--2558},\n  publisher = {{Proceedings of the National Academy of Sciences}},\n  doi = {10.1073/pnas.79.8.2554},\n  url = {https://www.pnas.org/doi/10.1073/pnas.79.8.2554},\n  urldate = {2022-08-11},\n}\n"},"uses":{},"slash":"always"},null];

					Promise.all([
						import("../_app/immutable/entry/start.eca8d74a.js"),
						import("../_app/immutable/entry/app.47f07e73.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 4],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
</body>
</html>
